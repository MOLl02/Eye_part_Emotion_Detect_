{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaTgh2P3QSVR"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "QAvDbjhU6hnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "iabMNfmd6hrf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8PfQ-5Wv7lk"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRzOg4rHQjBd"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip?dl=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjq4_ZAkShm5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras_preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
        "from keras.losses import categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqvG3quwZnZU"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "     zoom_range = 0.2, \n",
        "     shear_range = 0.2, \n",
        "     horizontal_flip=True, \n",
        "     rescale = 1./255\n",
        ")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(directory= \"/content/train\", \n",
        "                                               target_size=(224,224), \n",
        "                                               batch_size=32,\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmzes_0YZrVZ"
      },
      "outputs": [],
      "source": [
        "train_data.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-boMZu0zarzh"
      },
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255 )\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(directory= \"/content/test\", \n",
        "                                           target_size=(224,224), \n",
        "                                           batch_size=32,\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXt7WEPDtdkU"
      },
      "outputs": [],
      "source": [
        "#mobilenet module\n",
        "\n",
        "base_model = MobileNet( input_shape=(224,224,3), include_top= False )\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(units=7 , activation='softmax' )(x)\n",
        "\n",
        "# softmax should be proper... maybe?\n",
        "model = Model(base_model.input, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfFxmdE2tjtu"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRIh5nIstklA"
      },
      "outputs": [],
      "source": [
        "#showing some pictures\n",
        "t_img , label = train_data.next()\n",
        "\n",
        "def plotImages(img_arr, label):\n",
        "\n",
        "  count = 0\n",
        "  for im, l in zip(img_arr,label) :\n",
        "    plt.imshow(im)\n",
        "    \n",
        "    plt.title({i for i in train_data.class_indices if train_data.class_indices[i]==np.nonzero(l)[0][0]})\n",
        "    plt.axis = False\n",
        "    plt.show()\n",
        "    \n",
        "    count += 1\n",
        "    if count == 5:\n",
        "      break\n",
        "\n",
        "plotImages(t_img, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_zBBAyNN0H5"
      },
      "outputs": [],
      "source": [
        "# having early stopping and model check point \n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')\n",
        "\n",
        "mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')\n",
        "call_back = [es, mc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPDZKdfNN2Eb"
      },
      "outputs": [],
      "source": [
        "hist = model.fit_generator(train_data, \n",
        "                           steps_per_epoch= 10, \n",
        "                           epochs= 30, \n",
        "                           validation_data= val_data, \n",
        "                           validation_steps= 8, \n",
        "                           callbacks=[es,mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdUGSvgxsQEj"
      },
      "outputs": [],
      "source": [
        "# Loading the best fit model \n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "rvF4gD_r6xuu",
        "outputId": "9ec54b07-1c2c-46d4-fafc-02c1e77e2c82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
            "                                                                 \n",
            " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
            "                                                                 \n",
            " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
            "                                                                 \n",
            " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
            "                                                                 \n",
            " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
            "                                                                 \n",
            " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
            "                                                                 \n",
            " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
            "                                                                 \n",
            " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
            "                                                                 \n",
            " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
            "                                                                 \n",
            " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
            "                                                                 \n",
            " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
            "                                                                 \n",
            " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
            "                                                                 \n",
            " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
            "                                                                 \n",
            " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
            "                                                                 \n",
            " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
            "                                                                 \n",
            " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
            "                                                                 \n",
            " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
            "                                                                 \n",
            " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
            "                                                                 \n",
            " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
            "                                                                 \n",
            " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
            "                                                                 \n",
            " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 50176)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 351239    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,580,103\n",
            "Trainable params: 351,239\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LpbUY3wsVZj"
      },
      "outputs": [],
      "source": [
        "h =  hist.history\n",
        "h.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrHgexGRsWJv"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['accuracy'])\n",
        "plt.plot(h['val_accuracy'] , c = \"red\")\n",
        "plt.title(\"acc vs v-acc\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qwVp6mjsZRi"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['loss'])\n",
        "plt.plot(h['val_loss'] , c = \"red\")\n",
        "plt.title(\"loss vs v-loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrsespcu7n-b"
      },
      "outputs": [],
      "source": [
        "# just to map o/p values \n",
        "op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQp4IRH1Pepk"
      },
      "outputs": [],
      "source": [
        "# path for the image to see if it predics correct class\n",
        "\n",
        "path = \"/content/test/angry/PrivateTest_1488292.jpg\"\n",
        "img = load_img(path, target_size=(224,224) )\n",
        "\n",
        "i = img_to_array(img)/255\n",
        "input_arr = np.array([i])\n",
        "input_arr.shape\n",
        "\n",
        "pred = np.argmax(model.predict(input_arr))\n",
        "\n",
        "print(f\" the image is of {op[pred]}\")\n",
        "\n",
        "# to display the image  \n",
        "plt.imshow(input_arr[0])\n",
        "plt.title(\"input image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTTGm02ylNqF"
      },
      "outputs": [],
      "source": [
        "pip install face-recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or8otX7Vpuut"
      },
      "outputs": [],
      "source": [
        "import face_recognition\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pkmpe75bF1y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from keras_preprocessing.image import load_img, img_to_array \n",
        "from keras.models import  load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# load model\n",
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "\n",
        "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "path = \"/content/test_hreso/angry/images.jpg\"\n",
        "img = load_img(path, target_size=(224,224) )\n",
        "\n",
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "#ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
        "#test_img = img\n",
        "#gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "test_img = cv2.imread(path)\t\n",
        "gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "faces_detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK2t2Ve3QAF6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from keras_preprocessing.image import load_img, img_to_array \n",
        "from keras.models import  load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# load model\n",
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "#maybe dlib detector is better?\n",
        "#face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "path = \"/content/test_hreso/angry/images.jpg\"\n",
        "#img = load_img(path, target_size=(224,224) )\n",
        "\n",
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "#ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
        "#test_img = img\n",
        "#gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "detector_lm = dlib.get_frontal_face_detector()\n",
        "predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "test_img = cv2.imread(path)\t\n",
        "gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(gray_img)\n",
        "\n",
        "faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "rects = detector_lm(gray_img, 1)\n",
        "for rect in rects:\n",
        "    x = rect.tl_corner().x\n",
        "    y = rect.tl_corner().y\n",
        "    w = rect.width()\n",
        "    h = rect.height()\n",
        "    cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "    roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "    roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "    img_pixels = img_to_array(roi_gray)\n",
        "    img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "    img_pixels /= 255\n",
        "\n",
        "    predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "    max_index = np.argmax(predictions[0])\n",
        "\n",
        "    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "    predicted_emotion = emotions[max_index]\n",
        "\n",
        "    cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "    shape = predictor_lm(gray_img, rect)\n",
        "    shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "    for i in range(0, 68):\n",
        "        shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "    shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "    for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "        cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "\n",
        "#resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "#cv2_imshow(resized_img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRmnk_-DjjUq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgPNHzeMChM4"
      },
      "outputs": [],
      "source": [
        "# import the modules\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path/directory\n",
        "count = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "correct = 0\n",
        "TP_class = [0]*7\n",
        "FP_class = [0]*7\n",
        "FN_class = [0]*7\n",
        "\n",
        "table = np.arange(49).reshape((7,7))\n",
        "for emo in range(7):\n",
        "  folder_dir = \"/content/test_hreso/\" + op[emo] + \"/\"\n",
        "  print(folder_dir)\n",
        "  for images in os.listdir(folder_dir):\n",
        " \n",
        "    # check if the image ends with png\n",
        "    if (images.endswith(\".jpg\")):\n",
        "        test_img = cv2.imread(folder_dir+images)\n",
        "        #cv2_imshow(img)  \n",
        "        gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(gray_img)    \n",
        "        faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        if len(faces_detected) == 0:\n",
        "          continue\n",
        "        for (x, y, w, h) in faces_detected:\n",
        "          cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "          roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "          roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "          img_pixels = img_to_array(roi_gray)\n",
        "          img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "          img_pixels /= 255\n",
        "\n",
        "          predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "          max_index = np.argmax(predictions[0])\n",
        "          total += 1\n",
        "          table[emo,max_index]+=1\n",
        "          if max_index == emo:\n",
        "\n",
        "            correct += 1\n",
        "            TP_class[emo] += 1\n",
        "          else:\n",
        "            FP_class[max_index] += 1\n",
        "            FN_class[emo] += 1\n",
        "          emotions = ('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
        "          predicted_emotion = emotions[max_index]\n",
        "\n",
        "          cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "        cv2_imshow(resized_img)\n",
        "        \n",
        "        #count += 1\n",
        "        #if count > 5:\n",
        "        #  count = 0\n",
        "        #  break\n",
        "prec_class = [0]*7\n",
        "recl_class = [0]*7\n",
        "for emo in range(7):\n",
        "  trueP = table[emo,emo]\n",
        "  falseN = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseN += table[emo_i,emo]\n",
        "  falseP = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseP += table[emo,emo_i]\n",
        "  print(trueP)\n",
        "  print(trueP/falseN)\n",
        "  print(trueP/falseP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prec_class = [0]*7\n",
        "recl_class = [0]*7\n",
        "for emo in range(7):\n",
        "  trueP = table[emo,emo]\n",
        "  falseN = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseN += table[emo_i,emo]\n",
        "  falseP = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseP += table[emo,emo_i]\n",
        "  print(trueP)\n",
        "  prec_class[emo] = (trueP/(trueP+falseP))\n",
        "  recl_class[emo] = (trueP/(trueP+falseN))\n",
        "print(prec_class)\n",
        "print(recl_class)"
      ],
      "metadata": {
        "id": "eSzZW-XznmM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN1vMtaKLtpK"
      },
      "outputs": [],
      "source": [
        "# import the modules\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path/directory\n",
        "count = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "correct = 0\n",
        "TP_class = [0]*7\n",
        "FP_class = [0]*7\n",
        "FN_class = [0]*7\n",
        "detector_lm = dlib.get_frontal_face_detector()\n",
        "predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "table = np.arange(49).reshape((7,7))\n",
        "for emo in range(7):\n",
        "  folder_dir = \"/content/test_hreso/\" + op[emo] + \"/\"\n",
        "  print(folder_dir)\n",
        "  for images in os.listdir(folder_dir):\n",
        " \n",
        "    # check if the image ends with png\n",
        "    if (images.endswith(\".jpg\")):\n",
        "        test_img = cv2.imread(folder_dir+images)\n",
        "        #cv2_imshow(img)  \n",
        "        gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(gray_img)    \n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        #if len(faces_detected) == 0:\n",
        "        #cv2_imshow(gray_img)\n",
        "\n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        rects = detector_lm(gray_img, 1)\n",
        "        if len(rects) == 0:\n",
        "          continue\n",
        "        for rect in rects:\n",
        "          x = rect.tl_corner().x\n",
        "          y = rect.tl_corner().y\n",
        "          w = rect.width()\n",
        "          h = rect.height()\n",
        "          cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "          roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "          if roi_gray.size == 0:\n",
        "            continue\n",
        "          roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "          img_pixels = img_to_array(roi_gray)\n",
        "          img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "          img_pixels /= 255\n",
        "\n",
        "          predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "          max_index = np.argmax(predictions[0])\n",
        "          total += 1\n",
        "          table[emo,max_index]+=1\n",
        "          if max_index == emo:\n",
        "\n",
        "            correct += 1\n",
        "            TP_class[emo] += 1\n",
        "          else:\n",
        "            FP_class[max_index] += 1\n",
        "            FN_class[emo] += 1\n",
        "          emotions = ('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
        "          predicted_emotion = emotions[max_index]\n",
        "\n",
        "          cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "          shape = predictor_lm(gray_img, rect)\n",
        "          shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "          for i in range(0, 68):\n",
        "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "          shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "          for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "            cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "\n",
        "          resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "          cv2_imshow(resized_img)\n",
        "        \n",
        "        #count += 1\n",
        "        #if count > 5:\n",
        "        #  count = 0\n",
        "        #  break\n",
        "print(table)\n",
        "prec_class = [0]*7\n",
        "recl_class = [0]*7\n",
        "for emo in range(7):\n",
        "  trueP = table[emo,emo]\n",
        "  falseN = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseN += table[emo_i,emo]\n",
        "  falseP = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseP += table[emo,emo_i]\n",
        "  print(trueP)\n",
        "  prec_class[emo] = (trueP/(trueP+falseP))\n",
        "  recl_class[emo] = (trueP/(trueP+falseN))\n",
        "print(prec_class)\n",
        "print(recl_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "label_list = emotions\n",
        "num_list1 = prec_class\n",
        "num_list2 = recl_class\n",
        "x = range(7)\n",
        "\n",
        "rects1 = plt.bar(range(7), height=num_list1, width=0.4, alpha=0.8, color='red')\n",
        "rects2 = plt.bar([i + 0.4 for i in x], height=num_list2, width=0.4, color='green')\n",
        "plt.ylim(0, 1)    \n",
        "plt.ylabel(\"Performance\")\n",
        "plt.legend()  \n",
        "plt.xticks([index + 0.2 for index in x], label_list)\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.title(\"Performance Table\")\n",
        "for rect in rects1:\n",
        "    height = rect.get_height()\n",
        "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(round(height,2)), ha=\"center\", va=\"bottom\", label=\"precision\")\n",
        "for rect in rects2:\n",
        "    height = rect.get_height()\n",
        "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(round(height,2)), ha=\"center\", va=\"bottom\", label=\"recall\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TEhVL1IGnzxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6693Uhkh_vlR"
      },
      "outputs": [],
      "source": [
        "for emo in range(7):\n",
        "  #print(\"precision for \"+op[emo]+\" is: \"+ str(TP_class[emo]/(TP_class[emo]+FP_class[emo])))\n",
        "  print(str(TP_class[emo]) +\" true positive is identified in \"+str(TP_class[emo]+FP_class[emo])+\" Predicted Positives\")\n",
        "  #print(\"Recall for \"+op[emo]+\" is: \" +str(TP_class[emo]/(TP_class[emo]+FN_class[emo])))\n",
        "  print(str(TP_class[emo]) +\" true positive is identified in \"+str(TP_class[emo]+FN_class[emo])+\" Actual Positives\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGl44OH9Gxqd"
      },
      "outputs": [],
      "source": [
        "!unzip test_hreso.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCfybCxuVhoX"
      },
      "outputs": [],
      "source": [
        "def angle_between(p1, p2):\n",
        "    ang1 = np.arctan2(*p1[::-1])\n",
        "    ang2 = np.arctan2(*p2[::-1])\n",
        "    return np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
        "def draw_eye(p1,p2,input_img):\n",
        "  center_coordinates = (int((p1[0]+p2[0])/2), int((p1[1]+p2[1])/2))\n",
        "  \n",
        "  axesLength = (int(cv2.norm(p1,p2)*1.2), int(cv2.norm(p1,p2)*0.6))\n",
        "  \n",
        "  angle = angle_between(p1,p2)\n",
        "  \n",
        "  startAngle = 0\n",
        "  \n",
        "  endAngle = 360\n",
        "  color = (255, 255, 255)\n",
        "  thickness = -1\n",
        "\n",
        "  input_img = cv2.ellipse(input_img, center_coordinates, axesLength, angle, startAngle, endAngle, color, thickness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83W8tYD65szB"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "\n",
        "def eye_cut(path):\n",
        "  detector_lm = dlib.get_frontal_face_detector()\n",
        "  predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "  test_img = cv2.imread(path)\t\n",
        "  gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "  cv2_imshow(test_img)\n",
        "\n",
        "  faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "\n",
        "  rects = detector_lm(gray_img, 1)\n",
        "  for rect in rects:\n",
        "    test_img2 = test_img.copy()\n",
        "    test_img3 = test_img.copy()\n",
        "    x = rect.tl_corner().x\n",
        "    y = rect.tl_corner().y\n",
        "    w = rect.width()\n",
        "    h = rect.height()\n",
        "    cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "    roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "    roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "    img_pixels = img_to_array(roi_gray)\n",
        "    img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "    img_pixels /= 255\n",
        "\n",
        "    predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "    max_index = np.argmax(predictions[0])\n",
        "\n",
        "    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "    predicted_emotion = emotions[max_index]\n",
        "\n",
        "    cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "    shape = predictor_lm(gray_img, rect)\n",
        "    shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "    for i in range(0, 68):\n",
        "        shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "    shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "    for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "        cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "    eyel_l = shape[36]\n",
        "    eyel_r = shape[39]\n",
        "    eyer_l = shape[42]\n",
        "    eyer_r = shape[45]\n",
        "    mask_le = np.zeros(test_img2.shape, dtype=np.uint8)\n",
        "    mask_re = np.zeros(test_img2.shape, dtype=np.uint8)\n",
        "    draw_eye(eyel_l,eyel_r,mask_le)\n",
        "    draw_eye(eyer_l,eyer_r,mask_re)\n",
        "# Displaying the image\n",
        "    eyel_l1 = eyel_l\n",
        "    eyel_r1 = eyel_r\n",
        "    eyer_l1 = eyer_l\n",
        "    eyer_r1 = eyer_r\n",
        "    masked_image = cv2.bitwise_and(test_img2,mask_le)\n",
        "    cv2_imshow(masked_image)\n",
        "    masked_image2 = cv2.bitwise_and(test_img2,mask_re)\n",
        "    cv2_imshow(masked_image2)  \n",
        "\n",
        "    dst = cv2.addWeighted(masked_image, 0.5, masked_image2, 0.5, 0)\n",
        "    cv2_imshow(dst)  \n",
        "\n",
        "  #resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "  #cv2_imshow(resized_img)\n",
        "  return (eyel_l1,eyel_r1,eyer_l1,eyer_r1,masked_image,masked_image2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7Y3WIkVqJs"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "#path = \"/content/test_hreso/happy/images (8).jpg\"\n",
        "\n",
        "detector_lm = dlib.get_frontal_face_detector()\n",
        "predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "def eye_replace(path1,path2):\n",
        "  test_img = cv2.imread(path1)\t\n",
        "  gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "  cv2_imshow(test_img)\n",
        "\n",
        "  faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "\n",
        "\n",
        "  eyel_l1,eyel_r1,eyer_l1,eyer_r1,masked_image,masked_image2 = eye_cut(path2)\n",
        "\n",
        "  rects = detector_lm(gray_img, 1)\n",
        "  for rect in rects:\n",
        "    test_img2 = test_img.copy()\n",
        "    x = rect.tl_corner().x\n",
        "    y = rect.tl_corner().y\n",
        "    w = rect.width()\n",
        "    h = rect.height()\n",
        "    cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "    roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "    roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "    img_pixels = img_to_array(roi_gray)\n",
        "    img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "    img_pixels /= 255\n",
        "\n",
        "    predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "    max_index = np.argmax(predictions[0])\n",
        "\n",
        "    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "    predicted_emotion = emotions[max_index]\n",
        "\n",
        "    cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "    shape = predictor_lm(gray_img, rect)\n",
        "    shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "    for i in range(0, 68):\n",
        "        shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "    shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "    for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "        cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "    eyel_l = shape[36]\n",
        "    eyel_r = shape[39]\n",
        "    eyer_l = shape[42]\n",
        "    eyer_r = shape[45]\n",
        "\n",
        "    el_scaling = (cv2.norm(eyel_l,eyel_r) / cv2.norm(eyel_l1,eyel_r1))\n",
        "    print(el_scaling)\n",
        "\n",
        "    masked_imagel = masked_image.copy() \n",
        "    print(masked_image.shape[1]/test_img.shape[1])\n",
        "    print(eyel_l[0]/test_img.shape[1])\n",
        "    print(eyel_l1[0]/masked_image.shape[1])\n",
        "    print(eyel_r[0]/test_img.shape[1])\n",
        "    print(eyel_r1[0]/masked_image.shape[1])\n",
        "\n",
        "    width = int(masked_imagel.shape[1] * el_scaling)\n",
        "    height = int(masked_imagel.shape[0] * el_scaling)\n",
        "    dim = (width, height)\n",
        "  \n",
        "# resize image\n",
        "    masked_imagel = cv2.resize(masked_imagel, dim, interpolation = cv2.INTER_AREA)\n",
        " \n",
        "    mat_notrans=np.float32([[1,0,(eyel_l[0]-eyel_l1[0]*(el_scaling)+eyel_r[0]-eyel_r1[0]*(el_scaling))/2],[0,1,(eyel_l[1]-eyel_l1[1]*(el_scaling)+eyel_r[1]-eyel_r1[1]*(el_scaling))/2]])\n",
        "     \n",
        "    masked_imagel=cv2.warpAffine(masked_imagel,mat_notrans,(test_img.shape[1],test_img.shape[0]))\n",
        "\n",
        "    print(masked_imagel.shape)\n",
        "    cv2_imshow(masked_imagel)\n",
        "\n",
        "    gray_eyel = cv2.cvtColor(masked_imagel, cv2.COLOR_BGR2GRAY)\n",
        "    # make mask by thresholding sky image, antialias, convert to float in range 0 to 1 and make 3 channels\n",
        "    mask_eyel = cv2.threshold(gray_eyel, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
        "    mask_eyel = cv2.GaussianBlur(mask_eyel, (0,0), 2, 2)\n",
        "    mask_eyel[mask_eyel<128] = 0\n",
        "    mask_eyel = mask_eyel.astype(np.float32)/255\n",
        "    mask_eyel = cv2.merge([mask_eyel,mask_eyel,mask_eyel])\n",
        "\n",
        "# blend and convert back to 8-bit result\n",
        "    result = test_img2 * (1 - mask_eyel) + masked_imagel * mask_eyel\n",
        "    result = result.clip(0,255).astype(np.uint8)  \n",
        "    #cv2_imshow(result)\n",
        "\n",
        "\n",
        "\n",
        "    er_scaling = (cv2.norm(eyer_l,eyer_r) / cv2.norm(eyer_l1,eyer_r1))\n",
        "    print(er_scaling)\n",
        "\n",
        "    masked_imager = masked_image2.copy() \n",
        "    print(masked_image2.shape[1]/test_img.shape[1])\n",
        "    print(eyer_l[0]/test_img.shape[1])\n",
        "    print(eyer_l1[0]/masked_image2.shape[1])\n",
        "    print(eyer_r[0]/test_img.shape[1])\n",
        "    print(eyer_r1[0]/masked_image2.shape[1])\n",
        "\n",
        "    width = int(masked_imager.shape[1] * er_scaling)\n",
        "    height = int(masked_imager.shape[0] * er_scaling)\n",
        "    dim = (width, height)\n",
        "  \n",
        "# resize image\n",
        "    masked_imager = cv2.resize(masked_imager, dim, interpolation = cv2.INTER_AREA)\n",
        " \n",
        "    mat_notrans=np.float32([[1,0,(eyer_l[0]-eyer_l1[0]*(er_scaling)+eyer_r[0]-eyer_r1[0]*(er_scaling))/2],[0,1,(eyer_l[1]-eyer_l1[1]*(er_scaling)+eyer_r[1]-eyer_r1[1]*(er_scaling))/2]])\n",
        "     \n",
        "    masked_imager=cv2.warpAffine(masked_imager,mat_notrans,(test_img.shape[1],test_img.shape[0]))\n",
        "\n",
        "    print(masked_imager.shape)\n",
        "    cv2_imshow(masked_imager)\n",
        "\n",
        "    gray_eyer = cv2.cvtColor(masked_imager, cv2.COLOR_BGR2GRAY)\n",
        "    # make mask by thresholding sky image, antialias, convert to float in range 0 to 1 and make 3 channels\n",
        "    mask_eyer = cv2.threshold(gray_eyer, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
        "    mask_eyer = cv2.GaussianBlur(mask_eyer, (0,0), 2, 2)\n",
        "    mask_eyer[mask_eyer<128] = 0\n",
        "    mask_eyer = mask_eyer.astype(np.float32)/255\n",
        "    mask_eyer = cv2.merge([mask_eyer,mask_eyer,mask_eyer])\n",
        "\n",
        "# blend and convert back to 8-bit result\n",
        "    result = result * (1 - mask_eyer*0.9) + masked_imager * mask_eyer *0.9\n",
        "    result = result.clip(0,255).astype(np.uint8)  \n",
        "    cv2_imshow(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLOp0X9VDIAC"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "from os import path\n",
        "if path.exists('/content/test_edited') == False:\n",
        "  os.mkdir('/content/test_edited')\n",
        "for i in range(7):\n",
        "\n",
        "  if path.exists('/content/test_edited/'+op[i]) == False:\n",
        "\n",
        "    os.mkdir('/content/test_edited/'+op[i])\n",
        "\n",
        "  folder_dir = \"/content/test_hreso/\" + op[i] + \"/\"\n",
        "  out_dir = \"/content/test_edited/\" + op[i] + \"/\"\n",
        "\n",
        "  print(\"looking for eyes in folder\"+folder_dir)\n",
        "  for images_e in os.listdir(folder_dir):\n",
        "    print(\"looking for eyes in \"+images_e)\n",
        "    # check if the image ends with png\n",
        "    if (images_e.endswith(\".jpg\")):\n",
        "      for i1 in range(3):\n",
        "        i2 = random.randrange(7)\n",
        "        folder_dir2 = \"/content/test_hreso/\" + op[i2] + \"/\"\n",
        "        print(\"looking for heads in folder\"+folder_dir2)\n",
        "        try:\n",
        "      \n",
        "          images2 = random.choice(os.listdir(folder_dir2))\n",
        "          new_img = eye_replace(folder_dir2 + images2, folder_dir+images_e)\n",
        "          if (type(new_img) is np.ndarray): \n",
        "            cv2.imwrite(out_dir+images_e+images2, new_img)\n",
        "        except:\n",
        "          print(\"An exception occurred\")\n",
        "         \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMnZ_wbGZjFr"
      },
      "outputs": [],
      "source": [
        "eye_replace(\"/content/test_hreso/angry/download (3).jpg\", \"/content/test_hreso/disgust/download (3).jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op"
      ],
      "metadata": {
        "id": "it32itMaA_oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCTGuny_4E-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dxfhgQG4GEa"
      },
      "outputs": [],
      "source": [
        "# import the modules\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path/directory\n",
        "count = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "correct = 0\n",
        "TP_class = [0]*7\n",
        "FP_class = [0]*7\n",
        "FN_class = [0]*7\n",
        "detector_lm = dlib.get_frontal_face_detector()\n",
        "predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "table = np.arange(49).reshape((7,7))\n",
        "for emo in range(7):\n",
        "  folder_dir = \"/content/test_edited/\" + op[emo] + \"/\"\n",
        "  print(folder_dir)\n",
        "  for images in os.listdir(folder_dir):\n",
        " \n",
        "    # check if the image ends with png\n",
        "    if (images.endswith(\".jpg\")):\n",
        "        test_img = cv2.imread(folder_dir+images)\n",
        "        #cv2_imshow(img)  \n",
        "        gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(gray_img)    \n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        #if len(faces_detected) == 0:\n",
        "        #cv2_imshow(gray_img)\n",
        "\n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        rects = detector_lm(gray_img, 1)\n",
        "        if len(rects) == 0:\n",
        "          continue\n",
        "        for rect in rects:\n",
        "          x = rect.tl_corner().x\n",
        "          y = rect.tl_corner().y\n",
        "          w = rect.width()\n",
        "          h = rect.height()\n",
        "          cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "          roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "          if roi_gray.size == 0:\n",
        "            continue\n",
        "          roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "          img_pixels = img_to_array(roi_gray)\n",
        "          img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "          img_pixels /= 255\n",
        "\n",
        "          predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "          max_index = np.argmax(predictions[0])\n",
        "          total += 1\n",
        "          table[emo,max_index]+=1\n",
        "          if max_index == emo:\n",
        "\n",
        "            correct += 1\n",
        "            TP_class[emo] += 1\n",
        "          else:\n",
        "            FP_class[max_index] += 1\n",
        "            FN_class[emo] += 1\n",
        "          emotions = ('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
        "          predicted_emotion = emotions[max_index]\n",
        "\n",
        "          cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "          shape = predictor_lm(gray_img, rect)\n",
        "          shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "          for i in range(0, 68):\n",
        "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "          shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "          for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "            cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "\n",
        "          resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "          cv2_imshow(resized_img)\n",
        "        \n",
        "        #count += 1\n",
        "        #if count > 5:\n",
        "        #  count = 0\n",
        "        #  break\n",
        "print(table)\n",
        "prec_class = [0]*7\n",
        "recl_class = [0]*7\n",
        "for emo in range(7):\n",
        "  trueP = table[emo,emo]\n",
        "  falseN = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseN += table[emo_i,emo]\n",
        "  falseP = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseP += table[emo,emo_i]\n",
        "  print(trueP)\n",
        "  prec_class[emo] = (trueP/(trueP+falseP))\n",
        "  recl_class[emo] = (trueP/(trueP+falseN))\n",
        "print(prec_class)\n",
        "print(recl_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qb2lk30pY743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crIRjcCHY8Yo"
      },
      "outputs": [],
      "source": [
        "# import the modules\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path/directory\n",
        "count = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "correct = 0\n",
        "TP_class = [0]*7\n",
        "FP_class = [0]*7\n",
        "FN_class = [0]*7\n",
        "detector_lm = dlib.get_frontal_face_detector()\n",
        "predictor_lm = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "table = np.arange(49).reshape((7,7))\n",
        "for emo in range(7):\n",
        "  folder_dir = \"/content/test_s/\"\n",
        "  print(folder_dir)\n",
        "  for images in os.listdir(folder_dir):\n",
        " \n",
        "    # check if the image ends with png\n",
        "    if (images.endswith(\".png\")):\n",
        "        test_img = cv2.imread(folder_dir+images)\n",
        "        #cv2_imshow(img)  \n",
        "        gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(gray_img)    \n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        #if len(faces_detected) == 0:\n",
        "        #cv2_imshow(gray_img)\n",
        "\n",
        "        #faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "        rects = detector_lm(gray_img, 1)\n",
        "        if len(rects) == 0:\n",
        "          continue\n",
        "        for rect in rects:\n",
        "          x = rect.tl_corner().x\n",
        "          y = rect.tl_corner().y\n",
        "          w = rect.width()\n",
        "          h = rect.height()\n",
        "          cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=3)\n",
        "          roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "          if roi_gray.size == 0:\n",
        "            continue\n",
        "          roi_gray = cv2.resize(roi_gray, (224, 224))\n",
        "          img_pixels = img_to_array(roi_gray)\n",
        "          img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "          img_pixels /= 255\n",
        "\n",
        "          predictions = model.predict(img_pixels)\n",
        "\n",
        "      # find max indexed array\n",
        "          max_index = np.argmax(predictions[0])\n",
        "          total += 1\n",
        "          table[emo,max_index]+=1\n",
        "          if max_index == emo:\n",
        "\n",
        "            correct += 1\n",
        "            TP_class[emo] += 1\n",
        "          else:\n",
        "            FP_class[max_index] += 1\n",
        "            FN_class[emo] += 1\n",
        "          emotions = ('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
        "          predicted_emotion = emotions[max_index]\n",
        "\n",
        "          cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "          shape = predictor_lm(gray_img, rect)\n",
        "          shape_np = np.zeros((68, 2), dtype=\"int\")\n",
        "          for i in range(0, 68):\n",
        "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
        "          shape = shape_np\n",
        "\n",
        "        # Display the landmarks\n",
        "          for i, (x, y) in enumerate(shape):\n",
        "\t    # Draw the circle to mark the keypoint \n",
        "            cv2.circle(test_img, (x, y), 1, (0, 0, 255), -1)\n",
        "\n",
        "          resized_img = cv2.resize(test_img, (400, 400))\n",
        "\n",
        "          cv2_imshow(resized_img)\n",
        "        \n",
        "        #count += 1\n",
        "        #if count > 5:\n",
        "        #  count = 0\n",
        "        #  break\n",
        "print(table)\n",
        "prec_class = [0]*7\n",
        "recl_class = [0]*7\n",
        "for emo in range(7):\n",
        "  trueP = table[emo,emo]\n",
        "  falseN = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseN += table[emo_i,emo]\n",
        "  falseP = -trueP\n",
        "  for emo_i in range(7):\n",
        "    falseP += table[emo,emo_i]\n",
        "  print(trueP)\n",
        "  prec_class[emo] = (trueP/(trueP+falseP))\n",
        "  recl_class[emo] = (trueP/(trueP+falseN))\n",
        "print(prec_class)\n",
        "print(recl_class)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}